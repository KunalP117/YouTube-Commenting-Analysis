{"cells":[{"cell_type":"markdown","metadata":{"id":"0Z77ASJ43otk"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"VAhaipgAzbf1"},"source":["## Built on top of https://gist.github.com/AFAgarap/4f8a8d8edf352271fa06d85ba0361f26"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19481,"status":"ok","timestamp":1680920230785,"user":{"displayName":"Rational Inattention","userId":"03688276373612698902"},"user_tz":240},"id":"-cxXzprM8Ntc","outputId":"2ee5fa95-33f9-418a-b506-0a8e46079a63"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":38080,"status":"ok","timestamp":1680920268859,"user":{"displayName":"Rational Inattention","userId":"03688276373612698902"},"user_tz":240},"id":"ytUDfnpN6VOR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7fdaef17-5782-433c-f48c-6073fc9dc1c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of video thumbnails = 114724\n"]}],"source":["baseurl = '/content/drive/MyDrive/Datalist'\n","\n","import pickle\n","import os\n","## youtube module load from drive folder\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","##### IMPORT ALL THUMBNAIL IMAGES FROM YOUTUBE DATASET AND CONVERT TO GRAYSCALE ####\n","\n","os.chdir('/content/drive/MyDrive/')\n","## youtube module load from drive folder\n","import youtube.ytf\n","\n","gs_thumbnails = []\n","if os.path.isfile('thumbnails_grayscale_channel.pkl'): # if file exists and is saved, load the file\n","  with open('thumbnails_grayscale_channel.pkl', 'rb') as handle:\n","    gs_thumbnails_channel = pickle.load(handle)\n","    imgsize = (gs_thumbnails_channel[0].shape[1],gs_thumbnails_channel[0].shape[2])\n","\n","\n","else: # if 'thumbnails_grayscale.pkl' does not exist, pull from raw data and create file of grayscale images and save as 'thumbnails_grayscale.pkl'\n","  for filename in os.listdir(baseurl):\n","    f = os.path.join(baseurl, filename)\n","    if os.path.isfile(f):\n","      print('Unpacking file : '+baseurl + '/' + filename)\n","      ytfile = open(f, \"rb\")\n","      ytraw = pickle.load(ytfile)\n","      for vid in ytraw:\n","        if type(vid.image) is not list:\n","          gs_thumbnails.append(np.dot(vid.image, [0.299, 0.587, 0.114])) # convert image to grayscale\n","      ytfile.close()\n","\n","  # Convert each image to size (1,?,?)\n","\n","  imgsize = gs_thumbnails[0].shape\n","  gs_thumbnails_channel = np.array([elem.reshape((1,imgsize[0],imgsize[1])) for elem in gs_thumbnails])\n","  print(gs_thumbnails_channel[0].shape)  # ensure size is (1,40,80)\n","\n","  del vid,ytraw,ytfile, gs_thumbnails # delete these files for clearing up RAM\n","\n","  with open('thumbnails_grayscale_channel.pkl', 'wb') as handle:\n","    pickle.dump(gs_thumbnails_channel, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","num_videos = len(gs_thumbnails_channel)\n","print('Number of video thumbnails = {}'.format(num_videos))\n","\n","# check if shapes are consistent\n","assert (gs_thumbnails_channel[0].shape[1],gs_thumbnails_channel[0].shape[2]) == imgsize\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gDZ1m58KtMWo"},"source":["## Set training parameters, set up dataloader"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3694,"status":"ok","timestamp":1680920272532,"user":{"displayName":"Rational Inattention","userId":"03688276373612698902"},"user_tz":240},"id":"ph08ccs6zhql","outputId":"fcd0589a-3a52-4d8c-d037-a916e04eed78"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.utils.data.dataset.TensorDataset'>\n","torch.Size([256, 1, 40, 80])\n"]}],"source":["# Reproducibility\n","\n","seed = 42\n","torch.manual_seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","### Learning Parameters\n","\n","batch_size = 256\n","epochs = 2000\n","learning_rate = 1e-4\n","decay_rate = 0.975 # exponential decay for learning rate\n","\n","\n","# ## Load Datasets and create train and test data loaders\n","\n","val_frac = 0.2\n","indices = list(range(len(gs_thumbnails_channel)))\n","split = int(len(gs_thumbnails_channel)*val_frac)\n","\n","np.random.seed(1); np.random.shuffle(indices)\n","train_indices = indices[:split]\n","val_indices = indices[split:]\n","\n","\n","train_dataset = TensorDataset(torch.Tensor(gs_thumbnails_channel[train_indices])) # transform to torch tensor dataset\n","val_dataset  = TensorDataset(torch.Tensor(gs_thumbnails_channel[val_indices])) # transform to torch tensor dataset\n","\n","transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n","\n","#train_dataset = torchvision.datasets.MNIST(root=\"~/torch_datasets\", train=True, transform=transform, download=True\n","\n","# returns sampler and iterable with specified batch_size\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","\n","### Ensure train_dataset is a torch.utils.data.dataset.TensorDataset object\n","print(type(train_dataset))\n","\n","#### RAM Preservation\n","del train_indices, val_indices, split, indices, train_dataset, val_dataset\n","####\n","\n","### Ensure elem[0].shape is (batch_size,1,40,80)\n","count = 0\n","for elem in val_loader:\n","  count = count + 1\n","  if count == 2:\n","    break\n","  print(elem[0].shape)"]},{"cell_type":"markdown","metadata":{"id":"yeT_mR5N2aJj"},"source":["## Playground for Conv2D and ConvTranspose2D"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"GLa9jemq3hCy","executionInfo":{"status":"ok","timestamp":1680920272532,"user_tz":240,"elapsed":7,"user":{"displayName":"Rational Inattention","userId":"03688276373612698902"}}},"outputs":[],"source":["# import pickle\n","# import os\n","# ## youtube module load from drive folder\n","\n","# import matplotlib.pyplot as plt\n","# import numpy as np\n","\n","# import torch\n","# import torch.nn as nn\n","# import torch.optim as optim\n","# import torchvision\n","# from torch.utils.data import TensorDataset, DataLoader\n","\n","# x = torch.randn(2,8,20,40)\n","# conv = nn.Conv2d(8,16,3,padding = 1)\n","# convt = nn.ConvTranspose2d(16,8,3,padding = 1)\n","# maxpool = nn.MaxPool2d(kernel_size=2, return_indices=True)\n","# maxunpool = nn.MaxUnpool2d(kernel_size=2)\n","# # upsample = nn.UpsamplingNearest2d(size = (10,20))\n","\n","# print(x.shape)\n","# y = conv(x)\n","# print(y.shape)\n","# y,ind = maxpool(y)\n","# print(y.shape)\n","# xrecon = maxunpool(y,ind)\n","# print(xrecon.shape)\n","# xrecon = convt(xrecon)\n","# print(xrecon.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Hu7Xq3Mq3YTn","executionInfo":{"status":"ok","timestamp":1680920272533,"user_tz":240,"elapsed":7,"user":{"displayName":"Rational Inattention","userId":"03688276373612698902"}}},"outputs":[],"source":["# import pickle\n","# import os\n","# ## youtube module load from drive folder\n","\n","# import matplotlib.pyplot as plt\n","# import numpy as np\n","\n","# import torch\n","# import torch.nn as nn\n","# import torch.optim as optim\n","# import torchvision\n","# from torch.utils.data import TensorDataset, DataLoader\n","\n","# encoder_conv_layer_1 = nn.Sequential(\n","#           # Conv_L1 ImgIn shape=(?, 40, 80, 1)\n","#           # Conv -> (?, 40, 80, 8)\n","#           # Pool -> (?, 20, 40, 8)\n","#           nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1),\n","#           nn.ReLU()\n","#           )\n","# maxpool_1 = nn.MaxPool2d(kernel_size=2, return_indices=True)\n","# dropout_1 = nn.Dropout(p = 0.2)\n","\n","# encoder_conv_layer_2 = nn.Sequential(\n","#           # Conv_L2 ImgIn shape=(?, 20, 40, 8)\n","#           # Conv -> (?, 20, 40, 16)\n","#           # Pool -> (?, 10, 20, 16)\n","#           nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n","#           nn.ReLU()\n","#           )\n","# maxpool_2 = nn.MaxPool2d(kernel_size=2, padding = 1, return_indices=True)\n","# dropout_2 = nn.Dropout(p=0.2)\n","\n","# # encoder_conv_layer_3 = nn.Sequential(\n","# #           # Conv_L3 ImgIn shape=(?, 10, 20, 16)\n","# #           # Conv ->(?, 10, 20, 32)\n","# #           # Pool ->(?, 5, 10, 32)\n","# #           nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n","# #           nn.ReLU()\n","# #           )\n","# # dropout_3 = nn.Dropout(p=0.2)\n","\n","# flatten = nn.Flatten()\n","\n","# encoder_hidden_layer_1 = nn.Linear(\n","#           in_features=288, out_features=96\n","#       )\n","# encoder_hidden_layer_2 = nn.Linear(\n","#           in_features=96, out_features=24\n","#       )\n","# encoder_hidden_layer_3 = nn.Linear(\n","#           in_features=24, out_features=8\n","#       )\n","# encoder_output_layer = nn.Linear(\n","#           in_features=8, out_features=8\n","#       )\n","# decoder_hidden_layer_1 = nn.Linear(\n","#           in_features=8, out_features=24\n","#       )\n","# decoder_hidden_layer_2 = nn.Linear(\n","#           in_features=24, out_features=96\n","#       )\n","# decoder_output_layer = nn.Linear(\n","#           in_features=96, out_features=288\n","#       )\n","\n","# unflatten = nn.Unflatten(dim=1, unflattened_size = (16,3,6))\n","\n","# #deconv layers\n","# # decoder_conv_layer_1 = nn.Sequential(\n","# #           nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1), # exactly same as encoder_conv_layer_3\n","# #           nn.ReLU(),\n","# #           nn.Dropout(p = 0.2)\n","# #           )\n","\n","# maxunpool_1 = nn.MaxUnpool2d(kernel_size=2, padding = 1) \n","# decoder_conv_layer_1 = nn.Sequential(\n","#           # DeConv_L1 ImgIn shape=(?, 20, 40, 16)\n","#           # Deconv ->(?, 20, 40, 8)\n","#           nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1),\n","#           nn.BatchNorm2d(8),\n","#           nn.ReLU(),\n","#           nn.Dropout(p = 0.2)\n","#       )\n","\n","# maxunpool_2 = nn.MaxUnpool2d(kernel_size=2, padding = 1) # dimension = (?,40,80,8)\n","# decoder_conv_layer_2 = nn.Sequential(\n","#           # DeConv_L1 ImgIn shape=(?, 20, 40, 8)\n","#           # Deconv ->(?, 40, 80, 1)\n","#           nn.ConvTranspose2d(8, 1, kernel_size = 3, stride = 2, padding = 1)\n","# )\n","\n","\n","# x = torch.randn(10,1,40,80)\n","# print(x.shape)\n","# x = encoder_conv_layer_1(x)\n","# print(x.shape)\n","# x,ind1 = maxpool_1(x)\n","# print(x.shape)\n","# x = dropout_1(x)\n","# print(x.shape)\n","# x = encoder_conv_layer_2(x)\n","# print(x.shape)\n","# x,ind2 = maxpool_2(x)\n","# print(x.shape)\n","# x = dropout_2(x)\n","# print(x.shape)\n","# x = flatten(x)\n","# print(x.shape)\n","\n","# x = encoder_hidden_layer_1(x)\n","# print(x.shape)\n","# x = encoder_hidden_layer_2(x)\n","# print(x.shape)\n","# x = encoder_hidden_layer_3(x)\n","# print(x.shape)\n","# x = encoder_output_layer(x)\n","# x = torch.sigmoid(x)\n","# print(x.shape)\n","# x = decoder_hidden_layer_1(x)\n","# print(x.shape)\n","# x = decoder_hidden_layer_2(x)\n","# print(x.shape)\n","# x = decoder_output_layer(x)\n","# x = torch.sigmoid(x)\n","\n","\n","# print(x.shape)\n","# x = unflatten(x)\n","# print(x.shape)\n","# x = maxunpool_1(x,ind2)\n","# print(x.shape)\n","# x = decoder_conv_layer_1(x)\n","# print(x.shape)\n","\n","\n","\n","#####################################################\n","\n","# Autoencoder architecture with CNNs\n","# https://analyticsindiamag.com/how-to-implement-convolutional-autoencoder-in-pytorch-with-cuda/\n","# https://medium.com/dataseries/convolutional-autoencoder-in-pytorch-on-mnist-dataset-d65145c132ac\n","# https://towardsdatascience.com/convolutional-autoencoders-for-image-noise-reduction-32fce9fc1763\n","\n","# class AE_CNN(nn.Module):\n","#   def __init__(self, **kwargs):\n","#       super().__init__()\n","#       # THREE CONV LAYERS\n","#       self.encoder_conv_layer_1 = nn.Sequential(\n","#           # Conv_L1 ImgIn shape=(?, 40, 80, 1)\n","#           # Conv -> (?, 40, 80, 8)\n","#           # Pool -> (?, 20, 40, 8)\n","#           nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),\n","#           nn.ReLU()\n","#           )\n","#       self.maxpool_1 = nn.MaxPool2d(kernel_size=2, padding = 1, return_indices=True)\n","#       self.dropout_1 = nn.Dropout(p = 1 - kwargs['keep_prob'])\n","      \n","#       self.encoder_conv_layer_2 = nn.Sequential(\n","#           # Conv_L2 ImgIn shape=(?, 20, 40, 8)\n","#           # Conv -> (?, 20, 40, 16)\n","#           # Pool -> (?, 10, 20, 16)\n","#           nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n","#           nn.ReLU()\n","#           )\n","#       self.maxpool_2 = nn.MaxPool2d(kernel_size=2, stride=2, padding = 1, return_indices=True)\n","#       self.dropout_2 = nn.Dropout(p=1 - kwargs['keep_prob'])\n","      \n","#       self.encoder_conv_layer_3 = nn.Sequential(\n","#           # Conv_L3 ImgIn shape=(?, 10, 20, 16)\n","#           # Conv ->(?, 10, 20, 32)\n","#           # Pool ->(?, 5, 10, 32)\n","#           nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n","#           nn.ReLU()\n","#           )\n","#       self.maxpool_3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1, return_indices = True)\n","#       self.dropout_3 = nn.Dropout(p=1 - kwargs['keep_prob'])\n","\n","#       self.flatten = nn.Flatten()\n","\n","#       # Fully Connected Encoder and Decoder Layers\n","#       self.encoder_hidden_layer_1 = nn.Linear(\n","#           in_features=kwargs['input_shape'][0]*kwargs['input_shape'][1]*32//64, out_features=128\n","#       )\n","#       self.encoder_hidden_layer_2 = nn.Linear(\n","#           in_features=128, out_features=32\n","#       )\n","#       self.encoder_hidden_layer_3 = nn.Linear(\n","#           in_features=32, out_features=8\n","#       )\n","#       self.encoder_output_layer = nn.Linear(\n","#           in_features=8, out_features=8\n","#       )\n","#       self.decoder_hidden_layer_1 = nn.Linear(\n","#           in_features=8, out_features=32\n","#       )\n","#       self.decoder_hidden_layer_2 = nn.Linear(\n","#           in_features=32, out_features=128\n","#       )\n","#       self.decoder_output_layer = nn.Linear(\n","#           in_features=128, out_features=kwargs['input_shape'][0]*kwargs['input_shape'][1]*32//64\n","#       )\n","\n","#       # DeConv2D layers\n","#       #\n","#       # self.decoder_lin = nn.Sequential(\n","#       #     nn.Linear(kwargs['input_shape'][0]*kwargs['input_shape'][1]*128//64, 128),\n","#       #     nn.ReLU(True),\n","#       #     nn.Linear(128, 3 * 3 * 32),\n","#       #     nn.ReLU(True)\n","#       # )\n","\n","#       # Reshape into last conv layer's output dims\n","#       self.unflatten = nn.Unflatten(dim=1, \n","#       unflattened_size=(kwargs['input_shape'][0]//8,kwargs['input_shape'][1]//8,32) )\n","\n","#       self.maxunpool_1 = nn.MaxUnpool2d(kernel_size=2, stride = 2, padding = 1) # dimension = (?,10,20,32)\n","#       self.decoder_conv_layer_1 = nn.Sequential(\n","#           # DeConv_L1 ImgIn shape=(?, 10, 20, 32)\n","#           # Deconv ->(?, 10, 20, 16)\n","#           nn.ConvTranspose2d(32,16, kernel_size = 3, stride=1, padding=1), # exactly same as encodeer_conv_layer_3\n","#           nn.BatchNorm2d(16),\n","#           nn.ReLU(),\n","#           nn.Dropout(p = 1 - kwargs['keep_prob'])\n","#       )\n","\n","#       self.maxunpool_2 = nn.MaxUnpool2d(kernel_size=2, stride = 2, padding = 1) # dimension = (?,20,40,16)\n","#       self.decoder_conv_layer_2 = nn.Sequential(\n","#           # DeConv_L1 ImgIn shape=(?, 20, 40, 16)\n","#           # Deconv ->(?, 20, 40, 8)\n","#           nn.ConvTranspose2d(16, 8, kernel_size=3, stride=1, padding=1),\n","#           nn.BatchNorm2d(8),\n","#           nn.ReLU(),\n","#           nn.Dropout(p = 1 - kwargs['keep_prob'])\n","#       )\n","\n","#       self.maxunpool_3 = nn.MaxUnpool2d(kernel_size=2, stride = 2, padding = 1) # dimension = (?,40,80,8)\n","#       self.decoder_conv_layer_3 = nn.Sequential(\n","#           # DeConv_L1 ImgIn shape=(?, 20, 40, 8)\n","#           # Deconv ->(?, 40, 80, 1)\n","#           nn.ConvTranspose2d(8, 1, kernel_size = 3, stride = 1, padding = 1)\n","#       )\n","\n","\n","\n","#   def forward(self, features):\n","#     # convolution operations\n","#     activation = self.encoder_conv_layer_1(features)\n","#     activation,indices_1 = self.maxpool_1(activation)\n","#     activation = self.dropout_1(activation)\n","\n","#     activation = self.encoder_conv_layer_2(activation)\n","#     activation,indices_2 = self.maxpool_2(activation)\n","#     activation = self.dropout_2(activation)\n","\n","#     activation = self.encoder_conv_layer_3(activation)\n","#     activation,indices_3 = self.maxpool_3(activation)\n","#     activation = self.dropout_3(activation)\n","\n","#     # fully connected operations\n","#     activation = self.encoder_hidden_layer_1(activation)\n","#     activation = torch.relu(activation)\n","#     activation = self.encoder_hidden_layer_2(activation)\n","#     activation = torch.relu(activation)\n","#     activation = self.encoder_hidden_layer_3(activation)\n","#     activation = torch.relu(activation)\n","#     code = self.encoder_output_layer(activation)\n","#     code = torch.sigmoid(code)\n","#     activation = self.decoder_hidden_layer_1(code)\n","#     activation = torch.relu(activation)\n","#     activation = self.decoder_hidden_layer_2(activation)\n","#     activation = torch.relu(activation)\n","#     activation = self.decoder_output_layer(activation)\n","#     activation = torch.sigmoid(activation)\n","\n","\n","#     # convolution transpose operations\n","#     activation = self.unflatten(activation)\n","\n","#     activation = self.maxunpool_1(activation, indices_1)\n","#     activation = self.decoder_conv_layer_1(activation)\n","\n","#     activation = self.maxunpool_2(activation, indices_2)\n","#     activation = self.decoder_conv_layer_2(activation)\n","\n","#     activation = self.maxunpool_3(activation, indices_3)\n","#     reconstructed = self.decoder_conv_layer_3(activation)\n","\n","#     return reconstructed\n","    \n","#   # low dimensional embeddings\n","#   def embeddings(self,features):\n","#     # convolution operations\n","#     activation = self.encoder_conv_layer_1(activation)\n","#     activation,indices_1 = self.maxpool_1(activation)\n","#     activation = self.dropout_1(activation)\n","\n","#     activation = self.encoder_conv_layer_2(activation)\n","#     activation,indices_2 = self.maxpool_2(activation)\n","#     activation = self.dropout_2(activation)\n","\n","#     activation = self.encoder_conv_layer_3(activation)\n","#     activation,indices_3 = self.maxpool_3(activation)\n","#     activation = self.dropout_3(activation)\n","\n","#     # fully connected operations\n","#     activation = self.encoder_hidden_layer_1(activation)\n","#     activation = torch.relu(activation)\n","#     activation = self.encoder_hidden_layer_2(activation)\n","#     activation = torch.relu(activation)\n","#     activation = self.encoder_hidden_layer_3(activation)\n","#     activation = torch.relu(activation)\n","#     code = self.encoder_output_layer(activation)\n","#     code = torch.sigmoid(code)\n","#     return code\n","\n","\n","#####################################################"]},{"cell_type":"markdown","metadata":{"id":"5PI3R8pB4o2Z"},"source":["## Define Autoencoder Architectures, Check CUDA, Create Optimizer Object, Set criterion"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5919,"status":"ok","timestamp":1680920278447,"user":{"displayName":"Rational Inattention","userId":"03688276373612698902"},"user_tz":240},"id":"wsYeviMu4oI6","outputId":"bbd8ef7f-1552-4883-da13-6916623b5389"},"outputs":[{"output_type":"stream","name":"stdout","text":["<bound method Module.parameters of AE(\n","  (encoder_hidden_layer_1): Linear(in_features=3200, out_features=512, bias=True)\n","  (dropout1): Dropout(p=0.15, inplace=False)\n","  (encoder_hidden_layer_2): Linear(in_features=512, out_features=128, bias=True)\n","  (dropout2): Dropout(p=0.15, inplace=False)\n","  (encoder_hidden_layer_3): Linear(in_features=128, out_features=32, bias=True)\n","  (dropout3): Dropout(p=0.15, inplace=False)\n","  (encoder_hidden_layer_4): Linear(in_features=32, out_features=16, bias=True)\n","  (dropout4): Dropout(p=0.15, inplace=False)\n","  (encoder_output_layer): Linear(in_features=16, out_features=16, bias=True)\n","  (decoder_hidden_layer_1): Linear(in_features=16, out_features=32, bias=True)\n","  (decdropout1): Dropout(p=0.15, inplace=False)\n","  (decoder_hidden_layer_2): Linear(in_features=32, out_features=128, bias=True)\n","  (decdropout2): Dropout(p=0.15, inplace=False)\n","  (decoder_hidden_layer_3): Linear(in_features=128, out_features=512, bias=True)\n","  (decdropout3): Dropout(p=0.15, inplace=False)\n","  (decoder_output_layer): Linear(in_features=512, out_features=3200, bias=True)\n",")>\n"]}],"source":["# Autoencoder architecture without CNNs\n","class AE(nn.Module):\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","        self.encoder_hidden_layer_1 = nn.Linear(\n","            in_features=kwargs[\"input_shape\"], out_features=512\n","        )\n","        self.dropout1 = nn.Dropout(p = kwargs[\"drop_prob\"])\n","        self.encoder_hidden_layer_2 = nn.Linear(\n","            in_features=512, out_features=128\n","        )\n","        self.dropout2 = nn.Dropout(p = kwargs[\"drop_prob\"])\n","        self.encoder_hidden_layer_3 = nn.Linear(\n","            in_features=128, out_features=32\n","        )\n","        self.dropout3 = nn.Dropout(p = kwargs[\"drop_prob\"])\n","        self.encoder_hidden_layer_4 = nn.Linear(\n","            in_features=32, out_features=16\n","        )\n","        self.dropout4 = nn.Dropout(p = kwargs[\"drop_prob\"])\n","        self.encoder_output_layer = nn.Linear(\n","            in_features=16, out_features=16 # make change in assert statement after training process\n","        )\n","        self.decoder_hidden_layer_1 = nn.Linear(\n","            in_features=16, out_features=32\n","        )\n","        self.decdropout1 = nn.Dropout(p = kwargs[\"drop_prob\"])\n","        self.decoder_hidden_layer_2 = nn.Linear(\n","            in_features=32, out_features=128\n","        )\n","        self.decdropout2 = nn.Dropout(p = kwargs[\"drop_prob\"])\n","        self.decoder_hidden_layer_3 = nn.Linear(\n","            in_features=128, out_features=512\n","        )\n","        self.decdropout3 = nn.Dropout(p = kwargs[\"drop_prob\"])\n","        self.decoder_output_layer = nn.Linear(\n","            in_features=512, out_features=kwargs[\"input_shape\"]\n","        )\n","\n","    def forward(self, features):\n","        activation = self.encoder_hidden_layer_1(features)\n","        activation = torch.relu(activation)\n","        activation = self.dropout1(activation)\n","\n","        activation = self.encoder_hidden_layer_2(activation)\n","        activation = torch.relu(activation)\n","        activation = self.dropout2(activation)\n","\n","        activation = self.encoder_hidden_layer_3(activation)\n","        activation = torch.relu(activation)\n","        activation = self.dropout3(activation)\n","\n","        activation = self.encoder_hidden_layer_4(activation)\n","        activation = torch.relu(activation)\n","        activation = self.dropout4(activation)\n","\n","        code = self.encoder_output_layer(activation)\n","        code = torch.sigmoid(code)\n","        activation = self.decoder_hidden_layer_1(code)\n","        activation = torch.relu(activation)\n","        activation = self.decdropout1(activation)\n","\n","        activation = self.decoder_hidden_layer_2(activation)\n","        activation = torch.relu(activation)\n","        activation = self.decdropout2(activation)\n","\n","        activation = self.decoder_hidden_layer_3(activation)\n","        activation = torch.relu(activation)\n","        activation = self.decdropout3(activation)\n","\n","        activation = self.decoder_output_layer(activation)\n","        reconstructed = torch.sigmoid(activation)\n","        return reconstructed\n","    \n","    # low dimensional embeddings\n","    def embeddings(self,features):\n","      activation = self.encoder_hidden_layer_1(features)\n","      activation = torch.relu(activation)\n","      #activation = self.dropout1(activation)\n","\n","      activation = self.encoder_hidden_layer_2(activation)\n","      activation = torch.relu(activation)\n","      #activation = self.dropout2(activation)\n","\n","      activation = self.encoder_hidden_layer_3(activation)\n","      activation = torch.relu(activation)\n","      #activation = self.dropout3(activation)\n","\n","      activation = self.encoder_hidden_layer_4(activation)\n","      activation = torch.relu(activation)\n","      #activation = self.dropout4(activation)\n","\n","      code = self.encoder_output_layer(activation)\n","      code = torch.sigmoid(code)\n","      return code\n","\n","####################### \n","\n","#  use gpu if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# create a model from `AE` autoencoder class\n","# load it to the specified device, either gpu or cpu\n","\n","flattened_imgsize = imgsize[0]*imgsize[1]\n","model = AE(input_shape=flattened_imgsize, drop_prob = 0.15).to(device)\n","\n","#model = AE(input_shape=(1,40,80),keep_prob = 0.75).to(device) # Auto-encoder with conv encoder-decoder\n","\n","# create an optimizer object\n","# Adam optimizer with learning rate 1e-3\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# mean-squared error loss\n","criterion = nn.MSELoss()\n","\n","# learning rate scheduler\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma= decay_rate)\n","\n","# ensure you start and end with 40*80 number of features\n","print(model.parameters)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1680920278447,"user":{"displayName":"Rational Inattention","userId":"03688276373612698902"},"user_tz":240},"id":"Qhr_H4lE1doD"},"outputs":[],"source":["# # Visualize Neural Network\n","# !pip install torchviz\n","# from torchviz import make_dot\n","# # model = AE_CNN(input_shape=(1,40,80),keep_prob = 0.75).to(device) # Auto-encoder with dropout\n","# model = AE(input_shape=3200, drop_prob = 0.2).to(device) # Auto-encoder with dense layers only\n","# make_dot(model(torch.randn(1,3200).to(device)), params=dict(model.named_parameters()), show_attrs=True, show_saved=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"7G-GZwKqTyv3","executionInfo":{"status":"ok","timestamp":1680920278448,"user_tz":240,"elapsed":28,"user":{"displayName":"Rational Inattention","userId":"03688276373612698902"}}},"outputs":[],"source":["# class parent(object):\n","#   def __init__(self,arg1):\n","#     self.att1 = arg1\n","#   def shapeofatt(self):\n","#     return 'warning'\n","\n","# class child(parent):\n","#   def __init__(self,arg2):\n","#     #super().__init__([2,3])\n","#     self.att2 = arg2\n","#   def showfirst(self):\n","#     return self.att2[0]\n","\n","# childobj = child([1,0])\n","# childobj.shapeofatt()"]},{"cell_type":"code","source":["#del batch_features, val_features, outputs, voutputs, model, optimizer, criterion, train_loader, val_loader, gs_thumbnails, train_dataset, val_dataset, train_indices, val_indices, indices\n","import gc \n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"neNFXo9giiOA","executionInfo":{"status":"ok","timestamp":1680920278660,"user_tz":240,"elapsed":238,"user":{"displayName":"Rational Inattention","userId":"03688276373612698902"}},"outputId":"17e05be4-ce82-4e8a-a010-10eed71f76dd"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"7hTwfvUxpOmQ"},"source":["## Train with respect to set parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a6mfMXqZ0vhL","outputId":"e1f470f8-d449-4e96-daf4-fab1e769cd80"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch : 1/2000, recon loss = 0.08964081, val loss = 0.07951336\n","epoch : 51/2000, recon loss = 0.04780652, val loss = 0.04799373\n","epoch : 101/2000, recon loss = 0.04723851, val loss = 0.04760629\n","epoch : 151/2000, recon loss = 0.04279443, val loss = 0.04320178\n","epoch : 201/2000, recon loss = 0.04129151, val loss = 0.04159717\n","epoch : 251/2000, recon loss = 0.04096934, val loss = 0.04124769\n","epoch : 301/2000, recon loss = 0.04063924, val loss = 0.04085506\n","epoch : 351/2000, recon loss = 0.04037092, val loss = 0.04084807\n","epoch : 401/2000, recon loss = 0.04007038, val loss = 0.04035657\n","epoch : 451/2000, recon loss = 0.03966054, val loss = 0.04006924\n","epoch : 501/2000, recon loss = 0.03894858, val loss = 0.03922544\n","epoch : 551/2000, recon loss = 0.03844632, val loss = 0.03865421\n","epoch : 601/2000, recon loss = 0.03792514, val loss = 0.03833986\n","epoch : 651/2000, recon loss = 0.03768818, val loss = 0.03811654\n","epoch : 701/2000, recon loss = 0.03734724, val loss = 0.03770908\n","epoch : 751/2000, recon loss = 0.03711161, val loss = 0.03753333\n","epoch : 801/2000, recon loss = 0.03660475, val loss = 0.03700371\n","epoch : 851/2000, recon loss = 0.03636632, val loss = 0.03676828\n","epoch : 901/2000, recon loss = 0.03604146, val loss = 0.03652845\n","epoch : 951/2000, recon loss = 0.03590496, val loss = 0.03635359\n","epoch : 1001/2000, recon loss = 0.03588009, val loss = 0.03638411\n","epoch : 1051/2000, recon loss = 0.03559161, val loss = 0.03614503\n","epoch : 1101/2000, recon loss = 0.03550481, val loss = 0.03606076\n","epoch : 1151/2000, recon loss = 0.03540290, val loss = 0.03600711\n","epoch : 1201/2000, recon loss = 0.03536495, val loss = 0.03591134\n","epoch : 1251/2000, recon loss = 0.03528928, val loss = 0.03596862\n","epoch : 1301/2000, recon loss = 0.03509335, val loss = 0.03577952\n","epoch : 1351/2000, recon loss = 0.03507896, val loss = 0.03571636\n","epoch : 1401/2000, recon loss = 0.03509067, val loss = 0.03571059\n","epoch : 1451/2000, recon loss = 0.03498887, val loss = 0.03559911\n","epoch : 1501/2000, recon loss = 0.03488014, val loss = 0.03568250\n","epoch : 1551/2000, recon loss = 0.03482537, val loss = 0.03547738\n","epoch : 1601/2000, recon loss = 0.03473794, val loss = 0.03548346\n","epoch : 1651/2000, recon loss = 0.03474666, val loss = 0.03539762\n","epoch : 1701/2000, recon loss = 0.03472130, val loss = 0.03534783\n","epoch : 1751/2000, recon loss = 0.03454243, val loss = 0.03532197\n","epoch : 1801/2000, recon loss = 0.03446966, val loss = 0.03520706\n","epoch : 1851/2000, recon loss = 0.03443684, val loss = 0.03520262\n","epoch : 1901/2000, recon loss = 0.03442013, val loss = 0.03512592\n","epoch : 1951/2000, recon loss = 0.03421983, val loss = 0.03503344\n","Training finished. Total epochs: 2000\n"]}],"source":["\n","from copy import deepcopy\n","best_vloss = 10000.0\n","model_path = 'best_model_state_dict.pth' # standard naming convention for models\n","\n","\n","for epoch in range(epochs):\n","  loss = 0\n","  for batch_features in train_loader:\n","    # reshape mini-batch data to [N, 3200] matrix\n","    # load it to the active device\n","\n","    # flatten image for input\n","    batch_features = batch_features[0].view(-1, flattened_imgsize).to(device)\n","    \n","    # reset the gradients back to zero\n","    # PyTorch accumulates gradients on subsequent backward passes\n","    optimizer.zero_grad()\n","    \n","    # compute reconstructions\n","    outputs = model(batch_features)\n","    \n","    # compute training reconstruction loss\n","    train_loss = criterion(outputs, batch_features)\n","    \n","    # compute accumulated gradients\n","    train_loss.backward()\n","    \n","    # perform parameter update based on current gradients\n","    optimizer.step()\n","    \n","    # add the mini-batch training loss to epoch loss\n","    loss += train_loss.item()\n","    \n","  # compute the epoch training loss\n","  loss = loss/len(train_loader)\n","\n","  # display the epoch training loss\n","  if epoch%50 == 0: # print loss after every 20 epochs\n","    \n","\n","    vloss_avg = 0.0\n","\n","    for i, vdata in enumerate(val_loader):\n","      val_features = vdata[0].view(-1, flattened_imgsize).to(device)\n","      voutputs = model(val_features)\n","      vloss = criterion(voutputs, val_features)\n","      vloss_avg = (vloss_avg*i + vloss)/(i+1)\n","\n","    print(\"epoch : {}/{}, recon loss = {:.8f}, val loss = {:.8f}\".format(epoch+1, epochs, loss, vloss_avg))\n","\n","    # Track best performance, and save the model's state\n","    if vloss_avg < best_vloss:\n","        best_vloss = vloss_avg\n","        best_model_state_dict = model.state_dict()\n","        torch.save(best_model_state_dict, model_path) # save parameters of the model\n","        #### load up using model = TheModelClass(*args, **kwargs), model.load_state_dict(torch.load(PATH))\n","\n","print('Training finished. Total epochs: {}'.format(epochs))\n","##############\n","\n","\n","# load best model\n","model.load_state_dict(best_model_state_dict)\n","\n","\n","# Compute model embeddings - 16 dimensional representation of images\n","embeddings = []\n","with torch.no_grad(): # disable gradient accumulation, since model weights are frozen\n","  for batch_features in train_loader:\n","    batch_features = batch_features[0].view(-1, flattened_imgsize).to(device)\n","    # embeddings.append(model.embeddings(batch_features))\n","    embs = model.embeddings(batch_features)\n","    for emb in embs:\n","      embeddings.append(list(emb.cpu().numpy()))\n","  for batch_features in val_loader:\n","    batch_features = batch_features[0].view(-1, flattened_imgsize).to(device)\n","    embs = model.embeddings(batch_features)\n","    for emb in embs:\n","      embeddings.append(list(emb.cpu().numpy()))\n","\n","assert len(embeddings[0]) == 16 and len(embeddings) == num_videos\n","#assert list(embeddings[0].shape)[1] == 16 # ensure embedding is a low dim quantity\n","\n","os.chdir('/content/drive/MyDrive/')\n","# # save model embeddings () and save to drive location\n","with open('thumbnail_grayscale_embeddings.pkl','wb') as handle:\n","  pickle.dump(embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","\n","# # save model and optimizer state dictionaries\n","torch.save({'model_state_dict': best_model_state_dict, 'optimizer_state_dict': optimizer.state_dict()}, 'autoencoder_state_dicts.pth')\n","\n","\n","# # # DELETE ALL VARIABLES\n","# # del embeddings, batch_features, val_features, outputs, voutputs, criterion, train_loader, val_loader, gs_thumbnails\n","# del batch_features, val_features, outputs, voutputs, criterion, train_loader, val_loader, gs_thumbnails"]},{"cell_type":"markdown","metadata":{"id":"eOCZj6Sxsqp9"},"source":["## End session"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAh7-r1esskC"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["yeT_mR5N2aJj"],"provenance":[],"authorship_tag":"ABX9TyPxqAWWadT/vbqCyZ5tofSa"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}